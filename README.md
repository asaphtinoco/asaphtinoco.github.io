# Data & Cloud Engineer

### Articles
- [Cost Comparison: Databricks Cluster Jobs vs SQL Warehouse for Batch Processing](https://dev.to/asaphtinoco/cost-comparison-databricks-cluster-jobs-vs-sql-warehouse-for-batch-processing-gjg)
- [How Databricks Cluster Policies Can Enforce Good Behavior and Save You Money](https://dev.to/asaphtinoco/how-databricks-cluster-policies-can-enforce-good-behavior-and-save-you-money-4occ)
- [Optimizing Data Sync from PostgreSQL to Databricks with Fivetran](https://dev.to/asaphtinoco/optimizing-data-sync-from-postgresql-to-databricks-with-fivetran-5e9g)

### Projects 
| Project Name | Area | Description |
|---|---|---|
| [Terraform Modules](https://github.com/orgs/cloudconsulting-aws/repositories) | Cloud Infrastructure, DevOps | Designed and maintained reusable Terraform modules to standardize cloud resource provisioning, improve infrastructure consistency, and accelerate deployment workflows across multiple environments. |
| [Databricks Platform with Terraform](https://github.com/cloudconsulting-aws/terraform-databricks-platform) | Cloud Infrastructure, Automation | Automated the deployment and management of Databricks workspaces and resources using Terraform, ensuring reproducibility and scalability of cloud environments. |
| [Databricks Jobs Module](https://github.com/asaphtinoco/terraform-databricks-jobs) | Cloud Infrastructure, DevOps | Developed reusable Terraform modules to provision and manage Databricks jobs, streamlining CI/CD workflows and operational efficiency. |
| [Data Lakehouse with dbt + Spark + DuckDB](https://github.com/asaphtinoco/data-lakehouse-dbt-spark-duckdb) | Data Engineering, Analytics | Built a scalable data lakehouse architecture using dbt for transformations, Spark for distributed processing, and DuckDB for fast analytics and prototyping. |
| [Web3 On-chain Data Indexer (Ethereum/Cosmos)](https://github.com/asaphtinoco/web3-onchain-indexer) | Blockchain, Data Indexing | Developed an indexer to extract, process, and store on-chain data from Ethereum and Cosmos networks for analytics and reporting. |
| [Real-Time Analytics Pipeline with Kafka](https://github.com/asaphtinoco/realtime-analytics-kafka) | Streaming, Data Engineering | Implemented a real-time data pipeline using Kafka for event ingestion, processing, and analytics, enabling low-latency insights. |
| [Real-Time Streaming of Blockchain Events](https://github.com/asaphtinoco/blockchain-event-streaming) | Blockchain, Streaming | Engineered a system to capture and stream blockchain events in real time, supporting monitoring and alerting use cases. |
| [Index a Custom Token or NFT Collection](https://github.com/asaphtinoco/nft-token-indexer) | Blockchain, Data Indexing | Created a solution to index and analyze transactions and metadata for custom tokens or NFT collections, enabling search and visualization. |


## Skills
- **Cloud Platforms:** AWS, Google Cloud  
- **Infrastructure as Code:** Terraform, CloudFormation  
- **Containers & Orchestration:** Docker, Kubernetes  
- **ETL & Data Pipelines:** Apache Airflow, Fivetran, Airbyte Databricks, Apache Spark  
- **Programming Languages:** Python, SQL, Shell scripting, Go, PySpark  
- **CI/CD Tools:** Jenkins, GitLab CI, CircleCI, GitHub Actions, ArgoCD  
- **Databases:** PostgreSQL, MySQL, Redshift, Databricks  

## Work Experience

### Zepz — Senior Platform & Data Engineer (Freelance)  
*Remote | Apr 2023 – Apr 2025*  

- Built & maintained **scalable AWS data pipelines** to support global remittance operations.  
- Designed infrastructure with **Terraform (IaC)** for reproducible and secure environments.  
- Implemented **CI/CD automation** for infrastructure and data pipelines.  
- Optimized **Databricks ML workloads** in collaboration with Data Science teams.  
- Designed and provisioned Databricks platforms with Infrastructure as Code, implementing governance and compliance best practices.

---

### Sendwave (International Remittance) — Senior Data Engineer (Freelance)  
*Remote | Jan 2022 – Mar 2023*  

- Led migration from **GCP to AWS**, re-architecting services for scalability and security.  
- Improved **data pipelines** using Airflow, Spark, and containerized ETL processes.  
- Migrated BI platform from **Periscope to Looker**, improving analytics adoption.  
- Optimized **data models & pipelines** for performance and cost efficiency.  
- Provisioned **secure cloud platforms** and automated infrastructure deployments with Terraform.  

---

### Cloudnx — Director & Chief Engineer  
*Remote | 2022 – Present*  

- Founded **Cloudnx**, delivering **cloud & data engineering consulting** for fintech, remittance, and SaaS clients across Europe, US, and Brazil.  
- Designed and deployed **multi-cloud infrastructures** (AWS, Azure, GCP) using **Terraform (IaC)**.  
- Built **end-to-end CI/CD pipelines** (GitHub Actions, Jenkins, Airflow), reducing deployment times by **60%**.  
- Deployed **Databricks platforms & data pipelines**, improving data availability and reliability.  
- Drove **FinOps strategies**, cutting AWS costs by **up to 25%**.  
- Acted as both **technical lead & business owner**: client management, architecture reviews, and engineering.  

---

### Devoteam — Cloud & Platform Engineer  
*Lyon, France | Apr 2022 – Jun 2025*  

- Led **cloud build projects** at **CMA-CGM**, deploying AWS infrastructure at scale.  
- Built **platform foundations** (networking, IAM, CI/CD, monitoring) for secure and autonomous deployments.  
- Automated provisioning with **Terraform (IaC)**, reducing setup time by **70%**.  
- Standardized **infrastructure patterns & CI/CD templates** for platform engineering.  
- Integrated **observability tools** (CloudWatch, Datadog, Prometheus), strengthening resilience & compliance.  
- Worked with **DevOps & Security teams** to enforce governance and cost control.  

---

### Blu — Cloud Engineer  
*Rio de Janeiro, Brazil | Jun 2021 – Dec 2021*  

- Designed, deployed, and administered **AWS infrastructure** (EC2, VPC, RDS, DynamoDB, Lambda, EKS, Kafka).  
- Maintained **secure & scalable environments** across dev and production.  
- Implemented **IaC** with Terraform & Ansible, ensuring consistency and automation.  
- Built and optimized **CI/CD pipelines** with Jenkins, reducing release friction.  
- Monitored systems with **Datadog, Prometheus, CloudWatch** (alerting & observability dashboards).  
- Applied **FinOps practices** to reduce costs across multiple AWS accounts.  

---

### Blu — Data Engineer  
*Rio de Janeiro, Brazil | Jan 2020 – Jun 2021*  

- Translated business needs into **data solutions**, collaborating with stakeholders.  
- Designed and maintained **ETL processes & data warehouse architectures**.  
- Migrated BI platform from **Sisense to Looker**, improving reporting adoption & performance.  
- Built **data models & automated financial reporting pipelines**, reducing manual effort by **80%**.  
- Delivered **dashboards & KPIs** for C-level and business units, enabling data-driven decisions.  
- Contributed to **fraud detection & sales forecasting** via data prep and feature engineering.  
- Led the **Financial BI team**: governance, process mapping, and long-term strategy.  

---